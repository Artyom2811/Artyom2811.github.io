<!DOCTYPE HTML>
<html>
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
    <title>Apache Spark</title>
    <link rel="stylesheet" href="/style.css">
    <script src="/script.js"></script>
</head>

<body>
<div><a href="../BigData.html" class="backButton"></a></div>
<div class="bodyStyle">
    <h1>Apache Spark</h1>

    <div class="todo">
        <h2>Проработать:</h2>
        <ol>
            <li><a href="https://habr.com/ru/articles/339684/">Spark — Потрясающий веб-микрофреймворк для Java</a></li>
            <li><a href="https://www.youtube.com/watch?v=XLSQJQjmFFw">Евгений Борисов — Мифы о Spark, или Может ли пользоваться Spark обычный Java-разработчик</a></li>
            <li><a href="https://www.youtube.com/watch?v=JaWiugUMD6M">Какие задачи решает big data</a></li>
            <li><a href="https://habr.com/ru/company/otus/blog/575740/">Функции высшего порядка в Spark 3.1</a></li>
            <li><a href="https://habr.com/ru/company/piter/blog/417123/">Интеграция Spark Streaming и Kafka</a></li>
            <li><a href="https://habr.com/ru/articles/251507/">Apache Spark: что там под капотом?</a></li>
            <li><a href="https://habr.com/ru/companies/otus/articles/692800/">Машинное обучение с Apache Cassandra и Apache Spark</a></li>
            <li><a href="https://www.youtube.com/watch?v=If1xyQQEIvg">МИТАП: «Установка Apache Spark - это просто»_11 мая 2022г</a></li>
            <li><a href="https://www.youtube.com/watch?v=xuMe6OFyQ2s">МИТАП "Apache Spark за 2 часа - для нетерпеливых"_20 апреля 2022г</a></li>
            <li><a href="https://medium.com/@ongchengjie/different-types-of-spark-join-strategies-997671fbf6b0">Different Types of Spark Join Strategies</a></li>
            <li><a href="https://data-flair.training/blogs/spark-rdd-operations-transformations-actions/">Spark RDD Operations-Transformation & Action with Example</a></li>
            <li><a href="https://www.youtube.com/watch?v=3h6VWJLK31U">Что такое Apache Spark</a></li>
            <li>spark join strategies</li>
            <li>dataset vs dataframe spark</li>
            <li><a href="https://grouplens.org/datasets/movielens/">База данных Фильмов</a></li>
        </ol>
    </div>

    <section>
        <h2>Что это</h2>
        <p>Apache Spark — фреймворк с открытым исходным кодом для реализации распределённой обработки неструктурированных
            и слабоструктурированных данных через параллельные вычисления на кластере, входящий в экосистему проектов
            Hadoop.
        </p>
    </section>

    <section>
        <h2>Spark состоит:</h2>
        <ol>
            <li>SQL</li>
            <li>STREAMING DATA</li>
            <li>MACHINE LEARNING</li>
            <li>GRAPH ANALYTICS</li>
        </ol>
    </section>

    <section>
        <h3>RDD(Resilient Distributed Datasets)</h3>
        <p>RDD - основной базовая абстракция Spark, представляющая неизменный набор элементов, разделенных по узлам
            кластера, что позволяет выполнять параллельные вычисления.
        </p>

        <h3>Его можно получить из:</h3>
        <ol>
            <li>Файла</li>
            <li>Памяти</li>
            <li>Другого RDD</li>
        </ol>
        <p>SparkContext или JavaSparkContext(для JAVA) - место откуда мы получает RDD.</p>
        <p>Пример конфигурации JavaSparkContext:</p>
        <pre>
SparkConf conf = new SparkConf();
conf.setAppName("my spark application");
conf.setAppName("local[*]");
JavaSparkContext sc = new JavaSparkContext(conf);
        </pre>
    </section>

    <section>
        <h2>DataFrame</h2>
        <p>DataFrame - состоит из RDD.</p>
    </section>

    <section>
        <h2>Datasets</h2>
        <p>Datasets - это типизированные DataFrame.</p>
    </section>

    <section>
        <h2>RDD Operations:</h2>
        <p>Имеет методы, которые работают аналогично с Stream API(Transformations - Intermediate, Actions - Terminal).
            Также как в JAVA Stream API если метод возвращает RDD то это Transformations иначе это Actions.
        </p>

        <h3>Transformations:</h3>
        <ol>
            <li>map</li>
            <li>flatMap</li>
            <li>filter</li>
            <li>mapPartitions, mapPartitionsWithIndex</li>
            <li>sample</li>
            <li>union, intersection, join, cogroup, cartesian</li>
            <li>distinct</li>
            <li>reduceByKey, aggregateByKey, sortByKey</li>
            <li>pipe</li>
            <li>coalesce, repartition, repartitionAndSortWithinPartitions</li>
        </ol>

        <h3>Actions:</h3>
        <ol>
            <li>reduce</li>
            <li>collect</li>
            <li>count, countByKey, countByValue</li>
            <li>first</li>
            <li>take, takeSample, takeOrdered</li>
            <li>saveAsTextFile, saveAsSequenceFile, saveAsObjectFile</li>
            <li>foreach</li>
        </ol>

        <p>На Spark все Transformations методы выполняются на кластере и Actions на драйвере(тот кто отвечает за
            таксу и распределяет ее по кластеру).
        </p>

        <p>Мы можем сохранять промежуточные операции методом Transformations persist(StorageLevel).</p>
    </section>

    <section>
        <h2>DataFrame Operations:</h2>
        <ul>
            <li>show() - отображение контента DataFrame.
                <ul>show(boolean) - truncate.</ul>
                <ul>show(int) - number of rows.</ul>
            </li>
        </ul>
    </section>

    <section>
        <h2>Shuffle</h2>
        <p>Spark shuffle - это операция перемешивания(перераспределения ) данных между Экзекютерами.
            Побочный эффект таких аналитических преобразований, как join(), groupBy(), orderBy(),
            reduceByKey(), union() и тд.
        </p>

        <ol>Способы снижения отрицательного эффекта Shuffle-операций
            <li>Сократить число экзекюторов(меньше обмена между машинами)</li>
            <li>Уменьши объем данных(сбрасывания лишних столбцов, фильтрация)</li>
        </ol>
    </section>

    <section>
        <h2>Примеры кода</h2>
        <h3>Проверка колонки на максимальное значение(Date или Numeric)</h3>
        <pre>
.groupBy(col(AF.OfficeId)).agg(min(AF.Date))
        </pre>
    </section>

    <section>
        <h2>Catalyst Optimizer</h2>
        <p>Catalyst Optimizer - оптимизатор для DataSet.</p>
        <p>RDD - мы оптимизируем сами.</p>
    </section>
</div>
</body>
<script>setBackButtonHeight();</script>
</html>